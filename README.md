# NeoRetail: Policy-Gradient RL for Banner Position Optimization

**Track A: Offline Reinforcement Learning / Contextual Bandits**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Project Overview

This project implements a **REINFORCE-based policy gradient algorithm** to optimize banner ad position selection for maximizing click-through rate (CTR). Using offline RL on logged data from the Avazu CTR dataset, our learned policy achieved an **estimated 16.1% improvement** over the baseline.

### Key Results
- **Baseline CTR**: 16.97%
- **Learned Policy (IPS)**: 19.71%
- **Improvement**: +16.1% relative
- **Model**: 81.2M parameters
- **Training**: 5 epochs on 3.5M samples

---

## ğŸ“Š Results Summary

| Metric | Baseline | Our Policy | Improvement |
|--------|----------|------------|-------------|
| Validation IPS | 16.97% | **19.71%** | **+16.1%** |
| Validation WIS | 16.97% | **19.03%** | **+12.1%** |
| Test CTR | 16.97% | 16.97% | (logged data) |

**Peak Performance**: Epoch 3

---

## ğŸ—ï¸ Architecture
```
Input (20 features) 
    â†“
Embeddings (241 dims)
    â†“
Shared Trunk (256 â†’ 128)
    â†“
Policy Head (7 actions) + Value Head (baseline)
```

**Total Parameters**: 81,184,474

---

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install -r requirements.txt
```

### Training
```bash
python code/train_policy.py
```

### Evaluation
```bash
python code/plot_results.py
python code/test_analysis.py
```

---

## ğŸ“ Project Structure
```
project/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ train_policy.py          # Main training script
â”‚   â”œâ”€â”€ evaluate_policy.py       # Test evaluation
â”‚   â”œâ”€â”€ plot_results.py          # Generate plots
â”‚   â”œâ”€â”€ test_analysis.py         # Statistical analysis
â”‚   â””â”€â”€ fix_config.py            # Config utility
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ feature_config.json      # Feature configuration
â”‚   â”œâ”€â”€ action_encoder.pkl       # Action mapping
â”‚   â””â”€â”€ categorical_encoders.pkl # Feature encoders
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ training_curves.png      # Training visualization
â”‚   â”œâ”€â”€ test_analysis.png        # Test results
â”‚   â”œâ”€â”€ training_history.json    # Metrics per epoch
â”‚   â””â”€â”€ test_analysis.json       # Final results
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ report.pdf               # Full technical report
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”¬ Method

### Algorithm: REINFORCE + Baseline

Loss function:
```
L = -log Ï€(a|s)(R - V(s)) + 0.5(V(s) - R)Â² - Î²Â·H(Ï€)
```

Components:
- **Policy gradient**: Learns action selection
- **Value baseline**: Reduces variance
- **Entropy bonus**: Maintains exploration

### Off-Policy Evaluation

Since we work with logged data (offline RL), we estimate policy performance using:
- **IPS (Inverse Propensity Scoring)**: Unbiased estimate
- **WIS (Weighted Importance Sampling)**: Lower variance
- **Bootstrap CIs**: Confidence intervals

---

## ğŸ“ˆ Training Details

| Hyperparameter | Value |
|----------------|-------|
| Optimizer | Adam |
| Learning Rate | 1e-4 |
| Batch Size | 512 |
| Epochs | 5 |
| Entropy Coefficient | 0.01 |
| Gradient Clipping | 1.0 |

---

## ğŸ¯ Key Findings

1. **Position 6 is best** (32.89% CTR) but underutilized (0.1% of data)
2. **Position 0 is overused** (72.1% of data) despite low CTR (16.45%)
3. **Context matters**: Optimal position depends on device, site, time
4. **Policy learned** to shift traffic from position 0 â†’ position 6

---

## ğŸ“Š Visualizations

### Training Curves
![Training Curves](outputs/training_curves.png)

### Test Analysis
![Test Analysis](outputs/test_analysis.png)

---

## ğŸ”® Future Work

- [ ] Implement A2C/PPO for comparison
- [ ] Add Doubly Robust estimator
- [ ] Production deployment with A/B testing
- [ ] Online learning for distribution shift
- [ ] Model compression for faster inference

---

## ğŸ“š References

1. Williams (1992) - REINFORCE algorithm
2. Sutton & Barto (2018) - Reinforcement Learning textbook
3. DudÃ­k et al. (2014) - Doubly Robust evaluation
4. Avazu CTR Dataset - Kaggle

---

## ğŸ‘¥ Authors

**[Your Name]**  
[Your Email]  
[Course Name]

---

## ğŸ“„ License

MIT License - feel free to use for educational purposes!

---

## ğŸ™ Acknowledgments

- Avazu for the CTR dataset
- Course instructors for guidance
- PyTorch team for the framework

---

## ğŸ“§ Contact

Questions? Open an issue or contact [your.email@example.com]

---

**â­ If you found this project helpful, please give it a star!**
